import os
import zipfile
import requests
from pathlib import Path
import nltk

def load_nltk():
    """
    æ£€æŸ¥å¹¶åŠ è½½ NLTK ç¦»çº¿èµ„æºã€‚
    æ ¸å¿ƒé€»è¾‘ï¼šå°†èµ„æºè§£å‹åˆ° LlamaIndex å¼ºåˆ¶è¦æ±‚çš„ _static/nltk_cache å­ç›®å½•ä¸‹ã€‚
    """
    # 1. åŸºç¡€æ ¹ç›®å½•
    base_dir = Path("/mnt/workspace/llm_learn/nltk_data")
    
    # 2. ã€å…³é”®ã€‘LlamaIndex çœŸæ­£æœç´¢çš„ç‰©ç†æ·±å±‚ç›®å½•
    # å¦‚æœä¸è§£å‹åˆ°è¿™é‡Œï¼Œå®ƒå°±ä¼šæŠ¥ LookupError
    actual_cache_dir = base_dir / "_static" / "nltk_cache"
    
    # èµ„æºé…ç½®è¡¨
    resources = {
        "tokenizers/punkt": "https://www.modelscope.cn/datasets/haoznic/nltk_data_4_llm_learn/resolve/master/punkt.zip",
        "tokenizers/punkt_tab": "https://www.modelscope.cn/datasets/haoznic/nltk_data_4_llm_learn/resolve/master/punkt_tab.zip",
        "corpora/stopwords": "https://www.modelscope.cn/datasets/haoznic/nltk_data_4_llm_learn/resolve/master/stopwords.zip"
    }

    # 3. è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œå¯¹é½æœç´¢è·¯å¾„
    # å‘Šè¯‰ LlamaIndexï¼šç¼“å­˜æ ¹ç›®å½•æ˜¯ base_dir (å®ƒä¼šè‡ªåŠ¨å¾€åæ‹¼ _static/nltk_cache)
    os.environ["LLAMA_INDEX_CACHE_DIR"] = str(base_dir.resolve())
    # å‘Šè¯‰ NLTK åº“ï¼šç›´æ¥å»æœ€æ·±å±‚ç›®å½•æ‰¾
    os.environ["NLTK_DATA"] = str(base_dir.resolve())
    
    if str(base_dir) not in nltk.data.path:
        nltk.data.path.insert(0, str(base_dir.resolve()))

    # 4. å¾ªç¯æ£€æŸ¥å¹¶æŒ‰éœ€ä¸‹è½½
    actual_cache_dir.mkdir(parents=True, exist_ok=True)
    any_downloaded = False
    
    for sub_path, url in resources.items():
        # ç‰©ç†æ£€æŸ¥ç‚¹ï¼Œä¾‹å¦‚ï¼š.../nltk_data/_static/nltk_cache/tokenizers/punkt
        check_point = actual_cache_dir / sub_path
        
        if check_point.exists():
            continue
        
        any_downloaded = True
        zip_name = Path(url).name
        print(f"ğŸ“¦ æ­£åœ¨åˆå§‹åŒ–ç¦»çº¿èµ„æº: {zip_name}...")
        
        try:
            # å»ºç«‹çˆ¶ç›®å½• (ä¾‹å¦‚ tokenizers/ æˆ– corpora/)
            parent_dir = check_point.parent
            parent_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¸‹è½½ ZIP
            zip_file_path = parent_dir / zip_name
            response = requests.get(url, timeout=60, stream=True)
            response.raise_for_status()
            
            with open(zip_file_path, "wb") as f:
                for chunk in response.iter_content(chunk_size=1024*1024):
                    f.write(chunk)
            
            # è§£å‹åˆ°å½“å‰çˆ¶ç›®å½•
            with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
                zip_ref.extractall(parent_dir)
            
            zip_file_path.unlink()
            print(f"âœ… {zip_name} å¤„ç†æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ {zip_name} åˆå§‹åŒ–å¤±è´¥: {e}")

    if not any_downloaded:
        print(f"ğŸ’¡ ç¯å¢ƒæ£€æŸ¥ï¼šç¦»çº¿èµ„æºå·²åœ¨ {actual_cache_dir} å°±ç»ªã€‚")
    else:
        print(f"âœ… èµ„æºå·²æˆåŠŸè§£å‹è‡³å¯¹é½ç›®å½•ã€‚")


def load_key():
    import os
    import getpass
    import json
    import dashscope
    file_name = '../Key.json'
    if os.path.exists(file_name):
        with open(file_name, 'r') as file:
            Key = json.load(file)
        if "DASHSCOPE_API_KEY" in Key:
            os.environ['DASHSCOPE_API_KEY'] = Key["DASHSCOPE_API_KEY"].strip()
    else:
        DASHSCOPE_API_KEY = getpass.getpass("æœªæ‰¾åˆ°å­˜æ”¾Keyçš„æ–‡ä»¶ï¼Œè¯·è¾“å…¥ä½ çš„api_key:").strip()
        Key = {
            "DASHSCOPE_API_KEY": DASHSCOPE_API_KEY
        }
        # æŒ‡å®šæ–‡ä»¶å
        file_name = '../Key.json'
        with open(file_name, 'w') as json_file:
            json.dump(Key, json_file, indent=4)
        os.environ['DASHSCOPE_API_KEY'] = Key["DASHSCOPE_API_KEY"]
    dashscope.api_key = os.environ["DASHSCOPE_API_KEY"]
    
    # load_nltk()
    
if __name__ == '__main__':
    load_key()
    import os
    print(os.environ['DASHSCOPE_API_KEY'])
